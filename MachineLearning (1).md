# Machine Learning
# History
In 1642, one of the first mechanical adding machine was designed by Blaise Pascal. It used a system of gears and wheels similar to those found in odometers and other counting devices. Pascal's adder also known as Pascaline, could both add and subtract and was invented to calculate taxes.

In 1694 Gottfried Wilhelm Von Lebniez produced a similar machine to that of Pascaline, that was more accurate and could perform all four arithematic functions. Lebneiz also created binary system used by all modern computers. 
Storing data was the next challenge to be met. In 1801 the first use of storing data was in a weaving loom invented by Joseph Marie Jacquard that used metal cards punched with holes to position threads. A collection of these cards coded a program that directed the loom. This allowed for a process to be repeated with a consistent result every time.
In 1847, George Boole created a way of representing this using Boolean operators (AND, OR, NOR) and having responses represented by true or false, yes or no, and represented in binary as 1 or 0. Web searches still use these operators today.
In 1890 Herman Hollerith created the first combined system of mechanical calculation and punch cards to rapidly calculate statistics gathered from millions of people.
In 1945 Mark I built at IBM and designed by Howard Aiken, was the first combined electric and mechanical computer. The Mark I could store 72 numbers and it could perform complex multiplication in 6 seconds and division in 16.
In 1946 the first fully electronic computer was built by John Mauchly and John Eckert and named ENIAC, short for Electronic Numerical Integrator and Computer.
In 1952 Arthur Samuel was an IBM scientist who used the game of checkers to create the first learning program. His program became a better player after many games against itself and a variety of human players in a 'supervised learning mode'. The program observed which moves were winning strategies and adapted its programming to incorporate those strategies. 
In 1957 Frank Rosenblatt designed the perceptron which is a type of neural network. A neural network acts like your brain; the brain contains billions of cells called neurons that are connected together in a network. The perceptron connects a web of points where simple decisions are made that come together in the larger program to solve more complex problems.
In 1967 the first pattern regonition program were designed based on a type of algorithm called the nearest neighbour. An algorithm is a sequence of instructions and steps. When the program is given a new object it compares this with data from the training set and classifies the object to the nearest neighbour, or most similar object in memory.
In 1981 Gerald Dejong introduced explanation based learning, prior knowledge of the world is provided by training examples which makes this a type of supervised learning. The program analyzes the training data and discards irrelevant information to form a general rule to follow. For example in chess if the program is told that it needs to focus on the queen, it will discard all piesces that don't have immediate effect upon her.
In 1990's Machine learning applications in data mining, adaptive software and web applications, text learning , and language learning were started. Advances continued in machine learning algorithms within the general areas of supervised learning and unsupervised learning. As well, reinforcement learning algorithms were developed.
In 2000's the new millenium brought an explosion of adaptive programming. Anywhere adaptive programs are needed, machine learning learning is there. These programs are capable of recognizihng patterns, learning from experience, abstracting new information from data, and optimizing the efficiency and accuracy of its processing and output.




